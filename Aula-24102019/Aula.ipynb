{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importação das bibliotecas\n",
    "- Definição da variável data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as F\n",
    "date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDepartment = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_DEPARTMENT\")\n",
    "dfTemp = dfDepartment.select(\"DepartmentID\", \"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDepartment.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.orderBy(dfTemp[\"DepartmentID\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTemp.select(\"DepartmentID\").groupBy(\"DepartmentID\").count().orderBy(\"DepartmentID\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTempSql = spark.sql(\"\"\"select DepartmentID, count(*) \n",
    "                         from datalake01_refined.adv_aula_dbo_department \n",
    "                         group by DepartmentID \n",
    "                         order by DepartmentID\"\"\")\n",
    "dfTempSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DEPARTMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDepartment = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_DEPARTMENT\")\n",
    "dfDepartment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDepartment = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_DEPARTMENT/\")\n",
    "#df.show()\n",
    "primary_key = dfDepartment.select(\"PrimaryKey\").first()[0]\n",
    "#print(primary_key)\n",
    "dfAggDepartment = (dfDepartment.groupBy(primary_key).agg(F.max(\"ModifiedDate\").alias(\"ModifiedDate\")))\n",
    "dfAggDepartment.sort(\"DepartmentID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathDepartment = \"s3://datalake01-refined/ADV_AULA_DBO_DEPARTMENT/date_partition=\" + date\n",
    "dfDepartment.join(dfAggDepartment, dfAggDepartment.columns).coalesce(1).write.mode(\"append\").parquet(pathDepartment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EMPLOYEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmployee = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_EMPLOYEE\")\n",
    "dfEmployee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEmployee = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_EMPLOYEE/\")\n",
    "#df.show()\n",
    "primary_key = dfEmployee.select(\"PrimaryKey\").first()[0]\n",
    "#print(primary_key)\n",
    "dfAggEmployee = (dfEmployee.groupBy(primary_key).agg(F.max(\"ModifiedDate\").alias(\"ModifiedDate\")))\n",
    "dfAggEmployee.sort(\"BusinessEntityID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathEmployee = \"s3://datalake01-refined/ADV_AULA_DBO_EMPLOYEE/date_partition=\" + date\n",
    "dfEmployee.join(dfAggEmployee, dfAggEmployee.columns).coalesce(1).write.mode(\"append\").parquet(pathEmployee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPerson = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_PERSON\")\n",
    "dfPerson.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPerson = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_PERSON/\")\n",
    "#df.show()\n",
    "primary_key = dfPerson.select(\"PrimaryKey\").first()[0]\n",
    "#print(primary_key)\n",
    "dfAggPerson = (dfPerson.groupBy(primary_key).agg(F.max(\"ModifiedDate\").alias(\"ModifiedDate\")))\n",
    "dfAggPerson.sort(\"BusinessEntityID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathPerson = \"s3://datalake01-refined/ADV_AULA_DBO_PERSON/date_partition=\" + date\n",
    "dfPerson.join(dfAggPerson, dfAggPerson.columns).coalesce(1).write.mode(\"append\").parquet(pathPerson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EMPLOYEE DEPARTMENT HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistory = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_EMPLOYEEDEPARTMENTHISTORY\")\n",
    "dfHistory.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistory = spark.read.json(\"s3://datalake01-raw/ADV_AULA_DBO_EMPLOYEEDEPARTMENTHISTORY/\")\n",
    "#df.show()\n",
    "primary_key = dfHistory.select(\"PrimaryKey\").first()[0]\n",
    "#print(primary_key)\n",
    "dfAggHistory = (dfHistory.groupBy(primary_key).agg(F.max(\"ModifiedDate\").alias(\"ModifiedDate\")))\n",
    "dfAggHistory.sort(\"BusinessEntityID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathHistory = \"s3://datalake01-refined/ADV_AULA_DBO_EMPLOYEEDEPARTMENTHISTORY/date_partition=\" + date\n",
    "dfHistory.join(dfAggHistory, dfAggHistory.columns).coalesce(1).write.mode(\"append\").parquet(pathHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SPARK SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSQL = spark.sql(\"\"\"SELECT \n",
    "                    e.BusinessEntityID\n",
    "                    ,p.Title\n",
    "                    ,p.FirstName\n",
    "                    ,p.MiddleName\n",
    "                    ,p.LastName\n",
    "                    ,p.Suffix\n",
    "                    ,e.JobTitle\n",
    "                    ,d.Name AS Department\n",
    "                    ,d.GroupName\n",
    "                    ,edh.StartDate \n",
    "                FROM  datalake01_refined.adv_aula_dbo_employee  e\n",
    "                    INNER JOIN datalake01_refined.adv_aula_dbo_person  p\n",
    "                    ON p.BusinessEntityID = e.BusinessEntityID\n",
    "                    INNER JOIN datalake01_refined.adv_aula_dbo_employeedepartmenthistory  edh \n",
    "                    ON e.BusinessEntityID = edh.BusinessEntityID\n",
    "                    INNER JOIN datalake01_refined.adv_aula_dbo_department d \n",
    "                    ON edh.DepartmentID = d.DepartmentID\n",
    "                WHERE edh.EndDate IS NULL\"\"\")\n",
    "dfSQL.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
